{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BitcoinForecastApp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from kafka import KafkaConsumer\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "import traceback\n",
    "from utilities.timestamp_format import parse_timestamp, to_iso8601, format_timestamp\n",
    "from utilities.unified_config import get_service_config\n",
    "from utilities.data_utils import safe_round, filter_by_timestamp, normalize_timestamps, format_price\n",
    "from utilities.model_utils import safe_model_prediction, calculate_error_metrics\n",
    "import math\n",
    "\n",
    "# Add the models directory to the Python path\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__), '..'))\n",
    "from models.tfp_model import BitcoinForecastModel\n",
    "\n",
    "# Add imports for robust prediction\n",
    "from src.data_loader.instant_loader import InstantCSVLoader\n",
    "from src.features.instant_features import InstantFeatureExtractor\n",
    "from src.models.instant_model import InstantForecastModel\n",
    "from src.trainers.instant_trainer import InstantTrainer\n",
    "from utilities.logger import get_logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Class Definition and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BitcoinForecastApp:\n",
    "    \"\"\"\n",
    "    Orchestrates data loading, model fitting, forecasting, and evaluation for Bitcoin price forecasting.\n",
    "\n",
    "    This class manages configuration, data ingestion (from CSV/Kafka), model lifecycle, and prediction output.\n",
    "\n",
    "    :param config: Configuration dictionary\n",
    "    :return: BitcoinForecastApp instance\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Initialize the BitcoinForecastApp.\n",
    "\n",
    "        :param config: Configuration dictionary\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        \n",
    "         # Load data paths\n",
    "        self.data_file = self.config['data']['raw_data']['instant_data']['file']\n",
    "        self.predictions_file = self.config['data']['predictions']['instant_data']['predictions_file']\n",
    "        self.metrics_file = self.config['data']['predictions']['instant_data']['metrics_file']\n",
    "        \n",
    "        # Use environment variables as fallback for Kafka configuration\n",
    "        self.kafka_bootstrap_servers = os.getenv('KAFKA_BOOTSTRAP_SERVERS', \n",
    "                                               self.config['kafka']['bootstrap_servers'])\n",
    "        self.kafka_topic = os.getenv('KAFKA_TOPIC', \n",
    "                                   self.config['kafka']['topic'])\n",
    "        \n",
    "        # Ensure predictions directory exists\n",
    "        os.makedirs(os.path.dirname(self.predictions_file), exist_ok=True)\n",
    "        os.makedirs(os.path.dirname(self.metrics_file), exist_ok=True)\n",
    "        \n",
    "        # Initialization Kafka consumer with config settings\n",
    "        try:\n",
    "            self.consumer = KafkaConsumer(\n",
    "                self.kafka_topic,\n",
    "                bootstrap_servers=self.kafka_bootstrap_servers,\n",
    "                value_deserializer=lambda x: json.loads(x.decode('utf-8')),\n",
    "                **self.config['kafka']['consumer']\n",
    "            )\n",
    "            self.logger.info(f\"Initialized Kafka consumer for topic: {self.kafka_topic}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to initialize Kafka consumer: {e}\\n{traceback.format_exc()}\")\n",
    "            self.consumer = None\n",
    "        \n",
    "        # Initialize the TensorFlow Probability model\n",
    "        try:\n",
    "            self.model = BitcoinForecastModel(self.config)\n",
    "            self.logger.info(\"Successfully initialized TFP model\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to initialize model: {e}\")\n",
    "            self.model = None\n",
    "        \n",
    "        # Initialize last prediction time\n",
    "        self.last_prediction_time = None\n",
    "        \n",
    "        # Track last processed timestamp to prevent duplicate predictions\n",
    "        self.last_processed_second = None\n",
    "        \n",
    "        # Set window size for historical data from config\n",
    "        self.window_size = timedelta(seconds=self.config[self.service_name]['model']['instant']['window_size'])\n",
    "        \n",
    "        self.logger.info(f\"Initialized {self.config['app']['name']} v{self.config['app']['version']}\")\n",
    "        self.logger.info(f\"Data file: {self.data_file}\")\n",
    "        self.logger.info(f\"Predictions file: {self.predictions_file}\")\n",
    "        self.logger.info(f\"Metrics file: {self.metrics_file}\")\n",
    "        self.logger.info(f\"Kafka bootstrap servers: {self.kafka_bootstrap_servers}\")\n",
    "        self.logger.info(f\"Kafka topic: {self.kafka_topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 2. Timestamp and Data Loading Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_timestamp(self, dt: Any) -> str:\n",
    "    \"\"\"\n",
    "    Unified function to format timestamps to seconds precision.\n",
    "\n",
    "    :param dt: datetime object or timestamp string\n",
    "    :return: Formatted timestamp string in ISO8601 format with seconds precision\n",
    "    \"\"\"\n",
    "    return to_iso8601(dt)\n",
    "\n",
    "def load_historical_data(self) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load historical data from CSV file with windowing.\n",
    "\n",
    "    :return: DataFrame of historical price data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(self.data_file):\n",
    "            self.logger.warning(f\"Data file not found: {self.data_file}\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(\n",
    "            self.data_file,\n",
    "            names=self.config['data_format']['columns']['raw_data']['names'],\n",
    "            skiprows=1  # Skip header row\n",
    "        )\n",
    "        \n",
    "        if df.empty:\n",
    "            self.logger.warning(\"Data file is empty\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Convert timestamp to datetime and round to seconds\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "        df = df.dropna(subset=['timestamp'])  # Drop rows with invalid timestamps\n",
    "        \n",
    "        if df.empty:\n",
    "            self.logger.warning(\"No valid timestamps in data file\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Normalize timestamps for consistent timezone handling\n",
    "        df = normalize_timestamps(df, 'timestamp')\n",
    "        \n",
    "        # Filter to last window_size\n",
    "        cutoff_time = datetime.now().replace(microsecond=0) - self.window_size\n",
    "        # Ensure cutoff_time is timezone-aware (UTC)\n",
    "        cutoff_time = cutoff_time.replace(tzinfo=timezone.utc)\n",
    "        \n",
    "        # Use filter_by_timestamp utility for safe timestamp comparison\n",
    "        df = filter_by_timestamp(df, cutoff_time, 'timestamp')\n",
    "        \n",
    "        # Ensure numeric columns are float64\n",
    "        numeric_columns = self.config['data_format']['columns']['raw_data']['names'][1:]  # Skip timestamp\n",
    "        for col in numeric_columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        # Drop rows with NaN values\n",
    "        df = df.dropna()\n",
    "        \n",
    "        # Sort by timestamp\n",
    "        df = df.sort_values('timestamp')\n",
    "        \n",
    "        self.logger.info(f\"Loaded {len(df)} rows of historical data\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        self.logger.error(f\"Error loading historical data: {e}\\n{traceback.format_exc()}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def ensure_consistent_timestamp(self, timestamp: Any) -> str:\n",
    "    \"\"\"\n",
    "    Ensure consistent timestamp format for all operations.\n",
    "\n",
    "    :param timestamp: datetime object or timestamp string\n",
    "    :return: Standardized timestamp string\n",
    "    \"\"\"\n",
    "    # First ensure we have a datetime object\n",
    "    if isinstance(timestamp, str):\n",
    "        timestamp = parse_timestamp(timestamp)\n",
    "    \n",
    "    # Then format it consistently\n",
    "    if timestamp is not None:\n",
    "        # Ensure timezone is set\n",
    "        if timestamp.tzinfo is None:\n",
    "            timestamp = timestamp.replace(tzinfo=timezone.utc)\n",
    "        # Format with T separator\n",
    "        return format_timestamp(timestamp, use_t_separator=True)\n",
    "    \n",
    "    # Return current time as fallback\n",
    "    return format_timestamp(datetime.now(timezone.utc), use_t_separator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prediction and Metrics Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prediction(self, timestamp: Any, pred_price: float, pred_lower: float, pred_upper: float) -> bool:\n",
    "    \"\"\"\n",
    "    Save prediction to CSV file.\n",
    "\n",
    "    :param timestamp: Timestamp for the prediction\n",
    "    :param pred_price: Predicted price\n",
    "    :param pred_lower: Lower bound of prediction\n",
    "    :param pred_upper: Upper bound of prediction\n",
    "    :return: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(self.predictions_file), exist_ok=True)\n",
    "        \n",
    "        # Format timestamp consistently using ISO8601 format with T separator\n",
    "        timestamp_str = self.ensure_consistent_timestamp(timestamp)\n",
    "        \n",
    "        # Round price values to 2 decimal places using safe_round\n",
    "        pred_price = safe_round(pred_price, 2)\n",
    "        pred_lower = safe_round(pred_lower, 2)\n",
    "        pred_upper = safe_round(pred_upper, 2)\n",
    "        \n",
    "        # Check if file exists and needs header\n",
    "        file_exists = os.path.isfile(self.predictions_file)\n",
    "        if not file_exists or os.path.getsize(self.predictions_file) == 0:\n",
    "            # Create file with header\n",
    "            with open(self.predictions_file, 'w') as f:\n",
    "                f.write(\"timestamp,pred_price,pred_lower,pred_upper\\n\")\n",
    "            self.logger.info(f\"Created new predictions file with header\")\n",
    "        \n",
    "        # Format the line to write\n",
    "        line = f\"{timestamp_str},{pred_price},{pred_lower},{pred_upper}\\n\"\n",
    "        \n",
    "        # Write in append mode\n",
    "        with open(self.predictions_file, 'a') as f:\n",
    "            f.write(line)\n",
    "        \n",
    "        self.logger.info(f\"Saved prediction for {timestamp_str}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        self.logger.error(f\"Error saving prediction: {e}\\n{traceback.format_exc()}\")\n",
    "        return False\n",
    "\n",
    "def save_metrics(self, timestamp: Any, std: float, mae: float, rmse: float, actual_price: Optional[float] = None, pred_price: Optional[float] = None) -> bool:\n",
    "    \"\"\"\n",
    "    Save metrics to CSV file.\n",
    "\n",
    "    :param timestamp: Timestamp for the metrics\n",
    "    :param std: Standard deviation of prediction\n",
    "    :param mae: Mean absolute error\n",
    "    :param rmse: Root mean squared error\n",
    "    :param actual_price: Actual price (optional)\n",
    "    :param pred_price: Predicted price (optional)\n",
    "    :return: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(self.metrics_file), exist_ok=True)\n",
    "        \n",
    "        # Format timestamp consistently using ISO8601 format with T separator\n",
    "        timestamp_str = self.ensure_consistent_timestamp(timestamp)\n",
    "        \n",
    "        # Round metric values using safe_round\n",
    "        std = safe_round(std, 4)\n",
    "        mae = safe_round(mae, 4)\n",
    "        rmse = safe_round(rmse, 4)\n",
    "        \n",
    "        # Calculate actual error if both actual and predicted prices are available\n",
    "        actual_error = \"NA\"\n",
    "        if actual_price is not None and pred_price is not None:\n",
    "            actual_error = safe_round(actual_price - pred_price, 4)\n",
    "        \n",
    "        # Check if file exists and needs header\n",
    "        file_exists = os.path.isfile(self.metrics_file)\n",
    "        if not file_exists or os.path.getsize(self.metrics_file) == 0:\n",
    "            # Create file with header\n",
    "            with open(self.metrics_file, 'w') as f:\n",
    "                f.write(\"timestamp,std,mae,rmse,actual_error\\n\")\n",
    "            self.logger.info(f\"Created new metrics file with header\")\n",
    "        \n",
    "        # Format the line to write\n",
    "        line = f\"{timestamp_str},{std},{mae},{rmse},{actual_error}\\n\"\n",
    "        \n",
    "        # Write in append mode\n",
    "        with open(self.metrics_file, 'a') as f:\n",
    "            f.write(line)\n",
    "        \n",
    "        self.logger.info(f\"Saved metrics for {timestamp_str}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        self.logger.error(f\"Error saving metrics: {e}\\n{traceback.format_exc()}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prediction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(self, message_time: datetime, actual_price: float) -> bool:\n",
    "    \"\"\"\n",
    "    Make a prediction for the current timestamp.\n",
    "\n",
    "    :param message_time: Timestamp for the prediction\n",
    "    :param actual_price: Actual observed price\n",
    "    :return: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get historical data for model input\n",
    "        historical_data = self.load_historical_data()\n",
    "        if not historical_data.empty:\n",
    "            # Convert to numpy array for model input\n",
    "            price_series = historical_data['close'].values\n",
    "            \n",
    "            # Track model reinitialization attempts\n",
    "            model_reinit_count = getattr(self, 'model_reinit_count', 0)\n",
    "            max_reinit_attempts = 3\n",
    "            \n",
    "            try:\n",
    "                # Only update model if we have new data and it's time for an update\n",
    "                if self.last_prediction_time is None or \\\n",
    "                        (message_time - self.last_prediction_time).total_seconds() >= self.config[self.service_name]['model']['instant']['update_interval']:\n",
    "                    \n",
    "                    # Always pass the full price series to the model for better context\n",
    "                    self.logger.info(f\"Updating model with {len(price_series)} historical data points\")\n",
    "                    self.model.fit(price_series)\n",
    "                    self.last_prediction_time = message_time\n",
    "            \n",
    "                    # Reset reinit counter after successful update\n",
    "                    self.model_reinit_count = 0\n",
    "            \n",
    "            except Exception as model_error:\n",
    "                # Handle TensorFlow variable errors by reinitializing the model\n",
    "                if \"Unknown variable\" in str(model_error) or \"Variable not found\" in str(model_error):\n",
    "                    self.logger.warning(f\"TensorFlow variable error detected: {model_error}\")\n",
    "                    \n",
    "                    if model_reinit_count < max_reinit_attempts:\n",
    "                        self.logger.info(f\"Reinitializing model (attempt {model_reinit_count + 1}/{max_reinit_attempts})\")\n",
    "                        \n",
    "                        # Re-create the model from scratch\n",
    "                        self.model = BitcoinForecastModel(self.config)\n",
    "                        \n",
    "                        # Try fitting with the data again\n",
    "                        self.model.fit(price_series)\n",
    "                        \n",
    "                        # Increment the counter\n",
    "                        self.model_reinit_count = model_reinit_count + 1\n",
    "                        setattr(self, 'model_reinit_count', self.model_reinit_count)\n",
    "                        \n",
    "                        self.last_prediction_time = message_time\n",
    "                    else:\n",
    "                        self.logger.error(f\"Failed to reinitialize model after {max_reinit_attempts} attempts\")\n",
    "                        # Fall back to robust prediction\n",
    "                        return self.robust_prediction(message_time, actual_price)\n",
    "                else:\n",
    "                    # For other errors, log and continue with robust prediction\n",
    "                    self.logger.error(f\"Error updating model: {model_error}\\n{traceback.format_exc()}\")\n",
    "                    return self.robust_prediction(message_time, actual_price)\n",
    "            \n",
    "            try:\n",
    "                # Make prediction using safe prediction utility\n",
    "                pred_price, pred_lower, pred_upper = safe_model_prediction(\n",
    "                    model=self.model,\n",
    "                    method_name='forecast',\n",
    "                    fallback_value=actual_price  # Use actual price as fallback\n",
    "                )\n",
    "                \n",
    "                # Calculate standard deviation\n",
    "                std = (pred_upper - pred_lower) / 2\n",
    "                \n",
    "                # Use enhanced evaluation method from the model\n",
    "                try:\n",
    "                    eval_metrics = self.model.evaluate_prediction(\n",
    "                        actual_price=actual_price,\n",
    "                        prediction=pred_price,\n",
    "                        timestamp=message_time\n",
    "                    )\n",
    "                except Exception as eval_error:\n",
    "                    # Fallback to using our utility if model's evaluate_prediction fails\n",
    "                    self.logger.warning(f\"Error using model evaluation method: {eval_error}, using fallback\")\n",
    "                    eval_metrics = calculate_error_metrics(actual_price, pred_price)\n",
    "                    # Add any missing fields\n",
    "                    if 'z_score' not in eval_metrics:\n",
    "                        eval_metrics['z_score'] = 0.0\n",
    "                    if 'is_anomaly' not in eval_metrics:\n",
    "                        eval_metrics['is_anomaly'] = False\n",
    "                \n",
    "                # Get error metrics \n",
    "                mae = eval_metrics.get('absolute_error', abs(actual_price - pred_price))\n",
    "                \n",
    "                # Debug log with enhanced metrics\n",
    "                self.logger.info(\n",
    "                    f\"Prediction metrics: \"\n",
    "                    f\"Actual={format_price(actual_price)}, \"\n",
    "                    f\"Predicted={format_price(pred_price)}, \"\n",
    "                    f\"Error={format_price(actual_price - pred_price)}, \"\n",
    "                    f\"MAE={format_price(mae)}, \"\n",
    "                    f\"%Error={format_price(eval_metrics['percentage_error'])}%, \"\n",
    "                    f\"Z-score={format_price(eval_metrics['z_score'])}\"\n",
    "                )\n",
    "                \n",
    "                # Flag anomalous predictions for investigation\n",
    "                if eval_metrics['is_anomaly']:\n",
    "                    self.logger.warning(\n",
    "                        f\"ANOMALOUS PREDICTION DETECTED! Error Z-score: {format_price(eval_metrics['z_score'])} \"\n",
    "                        f\"exceeds threshold {self.model.anomaly_detection_threshold}\"\n",
    "                    )\n",
    "                \n",
    "                # Calculate RMSE (squared error)\n",
    "                rmse = math.sqrt((pred_price - actual_price) ** 2)\n",
    "                \n",
    "                # Log the prediction\n",
    "                self.logger.info(f\"Made prediction for timestamp {message_time.isoformat()}: Actual={format_price(actual_price)}, Predicted={format_price(pred_price)}\")\n",
    "                \n",
    "                # Save prediction to file\n",
    "                self.save_prediction(message_time, pred_price, pred_lower, pred_upper)\n",
    "                \n",
    "                # Save metrics to file with actual and predicted prices\n",
    "                self.save_metrics(message_time, std, mae, rmse, actual_price, pred_price)\n",
    "                \n",
    "                # Update model with the actual price for continuous learning\n",
    "                try:\n",
    "                    # Add the actual price to the end of the price series\n",
    "                    updated_series = np.append(price_series, actual_price)\n",
    "                    \n",
    "                    # Use the update method to incorporate the new observation\n",
    "                    self.model.update(updated_series[-60:])  # Use the last 60 points for efficiency\n",
    "                    self.logger.debug(\"Updated model with actual price for continuous learning\")\n",
    "                except Exception as update_error:\n",
    "                    self.logger.warning(f\"Could not update model with actual price: {update_error}\")\n",
    "            \n",
    "                return True\n",
    "            except Exception as pred_error:\n",
    "                self.logger.error(f\"Error making prediction: {pred_error}\\n{traceback.format_exc()}\")\n",
    "                return self.robust_prediction(message_time, actual_price)\n",
    "        else:\n",
    "            self.logger.warning(\"No historical data available for prediction\")\n",
    "            return self.robust_prediction(message_time, actual_price)\n",
    "    except Exception as e:\n",
    "        self.logger.error(f\"Error in make_prediction: {e}\\n{traceback.format_exc()}\")\n",
    "        # Return False to indicate failure and let the caller handle fallback\n",
    "        return self.robust_prediction(message_time, actual_price)\n",
    "\n",
    "def process_new_data(self, message: Any) -> None:\n",
    "    \"\"\"\n",
    "    Process new data from Kafka.\n",
    "\n",
    "    :param message: Kafka message containing new data\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = message.value\n",
    "        \n",
    "        # Properly parse timestamp based on format\n",
    "        if 'timestamp' not in data:\n",
    "            self.logger.error(\"Message missing 'timestamp' field\")\n",
    "            return\n",
    "            \n",
    "        # Get the timestamp from the message for reference\n",
    "        raw_timestamp = data['timestamp']\n",
    "        kafka_message_time = parse_timestamp(raw_timestamp)\n",
    "        if kafka_message_time is None:\n",
    "            self.logger.error(f\"Invalid timestamp format: {raw_timestamp}\")\n",
    "            return\n",
    "            \n",
    "        # Debug log to see actual timestamps\n",
    "        self.logger.info(f\"Processing Kafka message with raw timestamp: {raw_timestamp}\")\n",
    "        \n",
    "        # IMPORTANT: Always use current UTC time for the prediction timestamp\n",
    "        # This ensures predictions are always made for the current time\n",
    "        # regardless of when the data was collected\n",
    "        current_utc_time = datetime.now(timezone.utc).replace(microsecond=0)\n",
    "        message_time = current_utc_time\n",
    "        \n",
    "        # Standardize timestamp format for comparison by ensuring it's a proper ISO8601 string\n",
    "        # This fixes issues with timezone representation differences\n",
    "        current_second_str = format_timestamp(message_time.replace(microsecond=0), use_t_separator=True)\n",
    "        current_second = parse_timestamp(current_second_str)\n",
    "        \n",
    "        # Skip processing if we've already made a prediction for this second\n",
    "        if self.last_processed_second is not None:\n",
    "            last_second_str = format_timestamp(self.last_processed_second, use_t_separator=True)\n",
    "            if current_second_str == last_second_str:\n",
    "                self.logger.info(f\"Skipping duplicate prediction for second: {current_second_str}\")\n",
    "                return\n",
    "        \n",
    "        # Update last processed second with the standardized format\n",
    "        self.last_processed_second = current_second\n",
    "        \n",
    "        # Get price from either close or price field\n",
    "        if 'close' in data:\n",
    "            actual_price = float(data['close'])\n",
    "        elif 'price' in data:\n",
    "            actual_price = float(data['price'])\n",
    "        else:\n",
    "            self.logger.error(\"Message missing both 'close' and 'price' fields\")\n",
    "            return\n",
    "        \n",
    "        # Always log the actual timestamp we're working with with consistent formatting\n",
    "        self.logger.info(f\"Processing data for timestamp: {current_second_str}\")\n",
    "        \n",
    "        # Try processing with main prediction pipeline\n",
    "        try:\n",
    "            success = self.make_prediction(message_time, actual_price)\n",
    "            if success:\n",
    "                self._model_error_count = 0  # Reset model error counter on success\n",
    "            else:\n",
    "                # If main prediction fails cleanly, try fallback method\n",
    "                self.robust_prediction(message_time, actual_price)\n",
    "                \n",
    "        except ValueError as ve:\n",
    "            # Special handling for TensorFlow variable errors - likely need to reinitialize \n",
    "            if \"Unknown variable\" in str(ve) or \"optimizer can only be called for the variables\" in str(ve):\n",
    "                self.logger.error(f\"TensorFlow optimizer variable error: {ve}\")\n",
    "                self._model_error_count += 1\n",
    "                # Use robust prediction as fallback\n",
    "                self.robust_prediction(message_time, actual_price)\n",
    "            else:\n",
    "                raise  # Re-raise other ValueError exceptions\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in make_prediction: {e}\\n{traceback.format_exc()}\")\n",
    "            # Try fallback method if main fails\n",
    "            try:\n",
    "                self.robust_prediction(message_time, actual_price)\n",
    "            except Exception as fallback_error:\n",
    "                self.logger.error(f\"Fallback prediction also failed: {fallback_error}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        self.logger.error(f\"Error processing new data: {e}\\n{traceback.format_exc()}\")\n",
    "\n",
    "def robust_prediction(self, message_time: datetime, actual_price: float) -> bool:\n",
    "    \"\"\"\n",
    "    Fallback robust prediction using intelligent statistical methods.\n",
    "\n",
    "    :param message_time: Timestamp for the prediction\n",
    "    :param actual_price: Actual observed price\n",
    "    :return: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        self.logger.info(\"Using robust prediction fallback mechanism\")\n",
    "        \n",
    "        # Try to load data from the CSV file\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                self.data_file,\n",
    "                names=self.config['data_format']['columns']['raw_data']['names'],\n",
    "                skiprows=1\n",
    "            )\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading data for robust prediction: {e}\")\n",
    "            df = None\n",
    "        \n",
    "        if df is None or df.empty:\n",
    "            self.logger.warning(\"No data available for robust prediction\")\n",
    "            # If no data is available, use the actual price as our prediction\n",
    "            # with a small confidence interval\n",
    "            pred_price = actual_price\n",
    "            std_price = actual_price * 0.005  # 0.5% of actual price as std\n",
    "            lower_bound = actual_price - 1.96 * std_price\n",
    "            upper_bound = actual_price + 1.96 * std_price\n",
    "        else:\n",
    "            # Get the most recent data with proper windowing\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            \n",
    "            # Normalize timestamps for consistent timezone handling\n",
    "            df = normalize_timestamps(df, 'timestamp')\n",
    "            \n",
    "            # Sort by timestamp to ensure chronological order\n",
    "            df = df.sort_values('timestamp')\n",
    "            \n",
    "            # Try several window sizes for robustness with different techniques\n",
    "            windows = [1, 5, 15, 30, 60]  # minutes\n",
    "            window_weights = [0.4, 0.3, 0.15, 0.1, 0.05]  # higher weight for recent data\n",
    "            \n",
    "            predictions = []\n",
    "            \n",
    "            # 1. Simple Moving Average predictions\n",
    "            for i, window in enumerate(windows):\n",
    "                cutoff = message_time - timedelta(minutes=window)\n",
    "                # Ensure cutoff is timezone-aware\n",
    "                cutoff = cutoff.replace(tzinfo=timezone.utc)\n",
    "                # Use the utility function for timestamp filtering\n",
    "                window_df = filter_by_timestamp(df, cutoff, 'timestamp')\n",
    "                \n",
    "                if not window_df.empty and len(window_df) >= 3:  # Need at least 3 points\n",
    "                    prices = window_df['close'].values\n",
    "                    \n",
    "                    # Calculate moving average for this window\n",
    "                    window_size = min(len(prices), 10)\n",
    "                    if window_size > 1:\n",
    "                        # Use exponential weighting within this window\n",
    "                        inner_weights = np.exp(np.linspace(0, 1, window_size))\n",
    "                        inner_weights = inner_weights / inner_weights.sum()\n",
    "                        window_pred = np.average(prices[-window_size:], weights=inner_weights)\n",
    "                    else:\n",
    "                        window_pred = prices[-1]\n",
    "                    \n",
    "                    predictions.append((window_pred, window_weights[i], f\"MA-{window}min\"))\n",
    "            \n",
    "            # 2. Linear trend prediction\n",
    "            try:\n",
    "                # Use the last 30 minutes of data for trend analysis\n",
    "                trend_cutoff = message_time - timedelta(minutes=30)\n",
    "                # Ensure trend_cutoff is timezone-aware\n",
    "                trend_cutoff = trend_cutoff.replace(tzinfo=timezone.utc)\n",
    "                # Use the utility function for timestamp filtering\n",
    "                trend_df = filter_by_timestamp(df, trend_cutoff, 'timestamp')\n",
    "                \n",
    "                if len(trend_df) >= 5:  # Need at least 5 points for meaningful trend\n",
    "                    # Create a simple time index\n",
    "                    trend_df = trend_df.reset_index(drop=True)\n",
    "                    trend_df['time_idx'] = range(len(trend_df))\n",
    "                    \n",
    "                    # Fit a linear model\n",
    "                    from scipy import stats\n",
    "                    slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
    "                        trend_df['time_idx'], trend_df['close']\n",
    "                    )\n",
    "                    \n",
    "                    # Predict the next value\n",
    "                    next_idx = len(trend_df)\n",
    "                    trend_pred = slope * next_idx + intercept\n",
    "                    \n",
    "                    # Weight based on how good the linear fit is (r-squared)\n",
    "                    trend_weight = min(0.3, r_value**2)  # Cap at 0.3\n",
    "                    \n",
    "                    if not np.isnan(trend_pred) and abs(trend_pred - actual_price) < actual_price * 0.1:  # Sanity check\n",
    "                        predictions.append((trend_pred, trend_weight, \"Linear-Trend\"))\n",
    "                        self.logger.info(f\"Added linear trend prediction: {format_price(trend_pred)} (weight: {format_price(trend_weight)}, RÂ²: {format_price(r_value**2)})\")\n",
    "            except Exception as trend_error:\n",
    "                self.logger.warning(f\"Error calculating trend prediction: {trend_error}\")\n",
    "            \n",
    "            # 3. ARIMA prediction if statsmodels is available\n",
    "            try:\n",
    "                from statsmodels.tsa.arima.model import ARIMA\n",
    "                \n",
    "                # Use the last 60 minutes of data for ARIMA\n",
    "                arima_cutoff = message_time - timedelta(minutes=60)\n",
    "                # Ensure arima_cutoff is timezone-aware\n",
    "                arima_cutoff = arima_cutoff.replace(tzinfo=timezone.utc)\n",
    "                # Use the utility function for timestamp filtering\n",
    "                arima_df = filter_by_timestamp(df, arima_cutoff, 'timestamp')\n",
    "                \n",
    "                if len(arima_df) >= 10:  # Need sufficient data for ARIMA\n",
    "                    # Fit ARIMA model - simple (1,0,0) model for speed\n",
    "                    arima_model = ARIMA(arima_df['close'].values, order=(1,0,0))\n",
    "                    arima_result = arima_model.fit()\n",
    "                    \n",
    "                    # Forecast one step ahead\n",
    "                    arima_pred = arima_result.forecast(steps=1)[0]\n",
    "                    \n",
    "                    # Weight based on model AIC (lower is better)\n",
    "                    # Convert to a weight between 0 and 0.3\n",
    "                    aic = arima_result.aic\n",
    "                    arima_weight = 0.3  # Default weight\n",
    "                    \n",
    "                    if not np.isnan(arima_pred) and abs(arima_pred - actual_price) < actual_price * 0.1:  # Sanity check\n",
    "                        predictions.append((arima_pred, arima_weight, \"ARIMA\"))\n",
    "                        self.logger.info(f\"Added ARIMA prediction: {format_price(arima_pred)} (weight: {format_price(arima_weight)}, AIC: {format_price(aic)})\")\n",
    "            except (ImportError, Exception) as arima_error:\n",
    "                self.logger.debug(f\"Skipping ARIMA prediction: {arima_error}\")\n",
    "            \n",
    "            # 4. Add the actual price with a small weight as an anchor\n",
    "            predictions.append((actual_price, 0.1, \"Actual\"))\n",
    "            \n",
    "            # If we have any predictions, combine them with weights\n",
    "            if predictions:\n",
    "                # Log all predictions for debugging\n",
    "                formatted_predictions = [(format_price(p), format_price(w), m) for p, w, m in predictions]\n",
    "                self.logger.info(f\"Robust predictions: {formatted_predictions}\")\n",
    "                \n",
    "                # Normalize weights\n",
    "                total_weight = sum(w for _, w, _ in predictions)\n",
    "                if total_weight > 0:\n",
    "                    normalized_predictions = [(p, w/total_weight, m) for p, w, m in predictions]\n",
    "                    \n",
    "                    # Calculate weighted average\n",
    "                    pred_price = sum(p * w for p, w, _ in normalized_predictions)\n",
    "                else:\n",
    "                    # If weights sum to zero, use the actual price\n",
    "                    pred_price = actual_price\n",
    "            else:\n",
    "                # Fall back to the actual price if no predictions\n",
    "                pred_price = actual_price\n",
    "            \n",
    "            # Calculate volatility for confidence intervals based on recent data\n",
    "            recent_df = df.tail(30)  # Use last 30 data points for volatility\n",
    "            if len(recent_df) > 1:\n",
    "                # Calculate standard deviation\n",
    "                std_price = recent_df['close'].std()\n",
    "                \n",
    "                # If std is too small, use a percentage of the price\n",
    "                if std_price < 0.001 * pred_price:  # If std is too small (< 0.1% of price)\n",
    "                    std_price = 0.001 * pred_price  # Use 0.1% of price as minimum std\n",
    "                \n",
    "                # If std is too large, cap it\n",
    "                if std_price > 0.01 * pred_price:  # If std is too large (> 1% of price)\n",
    "                    std_price = 0.01 * pred_price  # Cap at 1% of price\n",
    "            else:\n",
    "                std_price = 0.005 * pred_price  # Default to 0.5% of price\n",
    "        \n",
    "        # Calculate confidence intervals (95%)\n",
    "        lower_bound = pred_price - 1.96 * std_price\n",
    "        upper_bound = pred_price + 1.96 * std_price\n",
    "        \n",
    "        # Use enhanced evaluation if model is available\n",
    "        if hasattr(self, 'model') and self.model is not None:\n",
    "            eval_metrics = self.model.evaluate_prediction(\n",
    "                actual_price=actual_price,\n",
    "                prediction=pred_price,\n",
    "                timestamp=message_time\n",
    "            )\n",
    "            mae = eval_metrics['absolute_error']\n",
    "            \n",
    "            # Log more detailed metrics\n",
    "            self.logger.info(\n",
    "                f\"Robust prediction metrics: \"\n",
    "                f\"Actual={format_price(actual_price)}, \"\n",
    "                f\"Predicted={format_price(pred_price)}, \"\n",
    "                f\"Error={format_price(actual_price - pred_price)}, \"\n",
    "                f\"MAE={format_price(mae)}, \"\n",
    "                f\"%Error={format_price(eval_metrics['percentage_error'])}%\"\n",
    "            )\n",
    "            \n",
    "            rmse = math.sqrt(mae ** 2)  # Simplified RMSE calculation\n",
    "        else:\n",
    "            # Fall back to simple metrics if model isn't available\n",
    "            mae = abs(actual_price - pred_price)\n",
    "            rmse = np.sqrt(mae ** 2)\n",
    "            self.logger.info(f\"Simple robust prediction metrics: Actual={format_price(actual_price)}, Predicted={format_price(pred_price)}, Error={format_price(actual_price - pred_price)}\")\n",
    "        \n",
    "        # Use the original timestamp from the message\n",
    "        # Log timestamp being used for prediction\n",
    "        self.logger.info(f\"Using message timestamp for robust prediction: {message_time.isoformat()}\")\n",
    "        \n",
    "        # Round predictions to 2 decimal places\n",
    "        pred_price = round(pred_price, 2)\n",
    "        lower_bound = round(lower_bound, 2)\n",
    "        upper_bound = round(upper_bound, 2)\n",
    "        \n",
    "        # Save prediction and metrics with the original message timestamp\n",
    "        self.save_prediction(message_time, pred_price, lower_bound, upper_bound)\n",
    "        self.save_metrics(message_time, std_price, mae, rmse, actual_price, pred_price)\n",
    "        \n",
    "        self.logger.info(f\"Made robust prediction for timestamp {message_time.isoformat()}: Actual={format_price(actual_price)}, Predicted={format_price(pred_price)}, Std={format_price(std_price)}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        self.logger.error(f\"Error in robust prediction: {e}\\n{traceback.format_exc()}\")\n",
    "        \n",
    "        # Even if everything fails, still try to save a reasonable prediction\n",
    "        try:\n",
    "            # Use the actual price with a small confidence interval\n",
    "            pred_price = actual_price\n",
    "            std_price = actual_price * 0.005  # 0.5% of price\n",
    "            lower_bound = actual_price - 1.96 * std_price\n",
    "            upper_bound = actual_price + 1.96 * std_price\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mae = 0.0  # Perfect prediction since we're using the actual price\n",
    "            rmse = 0.0  # Perfect prediction\n",
    "            \n",
    "            # Round predictions to 2 decimal places\n",
    "            pred_price = round(pred_price, 2)\n",
    "            lower_bound = round(lower_bound, 2)\n",
    "            upper_bound = round(upper_bound, 2)\n",
    "            \n",
    "            # Save this last-resort prediction\n",
    "            self.save_prediction(message_time, pred_price, lower_bound, upper_bound)\n",
    "            self.save_metrics(message_time, std_price, mae, rmse, actual_price, pred_price)\n",
    "            \n",
    "            self.logger.info(f\"Made last-resort prediction using actual price: {format_price(pred_price)}\")\n",
    "            return True\n",
    "        except Exception as final_err:\n",
    "            self.logger.error(f\"Final prediction attempt failed: {final_err}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 5. Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def run(self) -> None:\n",
    "        \"\"\"\n",
    "        Main loop to process new data and make predictions.\n",
    "\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Starting continuous predictions...\")\n",
    "        consecutive_errors = 0\n",
    "        max_consecutive_errors = 5\n",
    "        self._model_error_count = 0  # Track model-specific errors separately\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                # Check if Kafka consumer is working\n",
    "                if self.consumer is None:\n",
    "                    self.logger.warning(\"Kafka consumer not available. Trying to reconnect...\")\n",
    "                    try:\n",
    "                        self.consumer = KafkaConsumer(\n",
    "                            self.kafka_topic,\n",
    "                            bootstrap_servers=self.kafka_bootstrap_servers,\n",
    "                            value_deserializer=lambda x: json.loads(x.decode('utf-8')),\n",
    "                            **self.config['kafka']['consumer']\n",
    "                        )\n",
    "                        self.logger.info(\"Successfully reconnected to Kafka\")\n",
    "                    except Exception as e:\n",
    "                        self.logger.error(f\"Failed to reconnect to Kafka: {e}\")\n",
    "                        time.sleep(5)\n",
    "                        continue\n",
    "\n",
    "                # Try to get a message from Kafka with timeout\n",
    "                message = next(self.consumer, None)\n",
    "                if message:\n",
    "                    try:\n",
    "                        # Parse timestamp from message\n",
    "                        timestamp_str = message.value.get('timestamp')\n",
    "                        if timestamp_str:\n",
    "                            # Use our standardized timestamp function for consistency\n",
    "                            timestamp_str = self.ensure_consistent_timestamp(timestamp_str)\n",
    "                            # Parse using standardized format\n",
    "                            message_time = parse_timestamp(timestamp_str)\n",
    "                            \n",
    "                            if message_time:\n",
    "                                # Get current price from message\n",
    "                                current_price = None\n",
    "                                if 'close' in message.value:\n",
    "                                    current_price = float(message.value['close'])\n",
    "                                elif 'price' in message.value:\n",
    "                                    current_price = float(message.value['price'])\n",
    "                                    \n",
    "                                if current_price is not None:\n",
    "                                    # Update the message value with standardized timestamp\n",
    "                                    message.value['timestamp'] = timestamp_str\n",
    "                                    # Process the data\n",
    "                                    self.process_new_data(message)\n",
    "                                    consecutive_errors = 0\n",
    "                                else:\n",
    "                                    self.logger.warning(f\"Message missing price data: {message.value}\")\n",
    "                            else:\n",
    "                                self.logger.error(f\"Invalid timestamp in message: {timestamp_str}\")\n",
    "                        else:\n",
    "                            self.logger.error(\"Message missing timestamp\")\n",
    "                    except Exception as e:\n",
    "                        self.logger.error(f\"Error processing message: {e}\\n{traceback.format_exc()}\")\n",
    "                        consecutive_errors += 1\n",
    "                else:\n",
    "                    time.sleep(0.1)  # Small delay when no message is available\n",
    "\n",
    "                # Handle too many consecutive errors\n",
    "                if consecutive_errors >= max_consecutive_errors:\n",
    "                    self.logger.error(f\"Too many consecutive errors ({consecutive_errors}). Resetting consumer.\")\n",
    "                    try:\n",
    "                        if self.consumer:\n",
    "                            self.consumer.close()\n",
    "                        self.consumer = None\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    consecutive_errors = 0\n",
    "                    time.sleep(5)  # Wait before trying to reconnect\n",
    "                    \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error in main loop: {e}\\n{traceback.format_exc()}\")\n",
    "                time.sleep(5)  # Wait before retry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# BitcoinForecastModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "import traceback\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Class Definition and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BitcoinForecastModel:\n",
    "    \"\"\"\n",
    "    Core TensorFlow Probability model for Bitcoin price forecasting.\n",
    "\n",
    "    Implements a structural time series model with local linear trend, seasonal components,\n",
    "    day-of-week effects, and autoregressive parts. Includes data preprocessing, technical indicators,\n",
    "    outlier detection, and robust fallback mechanisms for prediction stability.\n",
    "\n",
    "    :param config: Configuration dictionary\n",
    "    :return: BitcoinForecastModel instance\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Initialize the Bitcoin forecast model.\n",
    "\n",
    "        :param config: Configuration dictionary\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.service_name = os.environ.get(\n",
    "            'SERVICE_NAME', 'bitcoin_forecast_app')\n",
    "\n",
    "        # Get model config from the service-specific section directly\n",
    "        model_config = None\n",
    "\n",
    "        # Check if service-specific config exists at top level and has model section\n",
    "        if self.service_name in self.config and 'model' in self.config[self.service_name]:\n",
    "            model_config = self.config[self.service_name]['model']['instant']\n",
    "            print(f\"Using service-specific model config from top level\")\n",
    "        else:\n",
    "            # Fallback to global model config if service-specific not found\n",
    "            model_config = self.config.get('model', {}).get('instant', {})\n",
    "            print(f\"Using fallback global model config\")\n",
    "\n",
    "        # If we still don't have a valid config, use defaults\n",
    "        if not model_config:\n",
    "            print(f\"No model config found, using defaults\")\n",
    "            model_config = {}\n",
    "\n",
    "        self.num_timesteps = model_config.get('lookback', 60)\n",
    "        self.num_seasons = model_config.get('num_seasons', 24)\n",
    "        self.model = None\n",
    "        self.posterior = None\n",
    "        self.observed_time_series = None\n",
    "        self.preprocessed_data = None\n",
    "\n",
    "        # Set default dtype to float64 for all tensors\n",
    "        tf.keras.backend.set_floatx('float64')\n",
    "        \n",
    "        # Store the learning rate from config\n",
    "        self.learning_rate = model_config.get('learning_rate', 0.01)\n",
    "\n",
    "        # Get VI steps from config\n",
    "        self.vi_steps = model_config.get('vi_steps', 100)\n",
    "\n",
    "        # # Add missing attributes for history size management\n",
    "        # self.max_history_size = model_config.get('max_history_size', 1000)\n",
    "        # self.min_points_req = model_config.get('min_points_req', 10)\n",
    "        # self.num_variational_steps = model_config.get('vi_steps', 100)\n",
    "        # Store num_samples for forecasting\n",
    "        self.num_samples = model_config.get('num_samples', 50)\n",
    "\n",
    "        # Advanced model parameters with defaults\n",
    "        # MCMC is more accurate but slower\n",
    "        self.use_mcmc = model_config.get('use_mcmc', False)\n",
    "        self.mcmc_steps = model_config.get('mcmc_steps', 1000)\n",
    "        self.mcmc_burnin = model_config.get('mcmc_burnin', 300)\n",
    "        self.use_day_of_week = model_config.get('use_day_of_week', True)\n",
    "        self.use_technical_indicators = model_config.get(\n",
    "            'use_technical_indicators', True)\n",
    "\n",
    "        # For technical indicators\n",
    "        self.short_ma_window = model_config.get('short_ma_window', 5)\n",
    "        self.long_ma_window = model_config.get('long_ma_window', 20)\n",
    "        self.volatility_window = model_config.get('volatility_window', 10)\n",
    "\n",
    "        # Track model rebuilds\n",
    "        self.model_version = 0\n",
    "\n",
    "        # Last forecast values (for fallback)\n",
    "        self.last_forecast = None\n",
    "        self.last_mean = None\n",
    "        self.last_lower = None\n",
    "        self.last_upper = None\n",
    "\n",
    "        # For evaluation\n",
    "        self.recent_errors = []\n",
    "        self.max_error_history = 100\n",
    "        self.anomaly_detection_threshold = 3.0  # Z-score threshold\n",
    "\n",
    "        # Setup TensorFlow function caching to prevent repeated retracing\n",
    "        self._setup_tf_function_caching()\n",
    "\n",
    "        # Debug log\n",
    "        print(\n",
    "            f\"Initialized model with num_samples={self.num_samples}, vi_steps={self.vi_steps}\")\n",
    "        if self.use_mcmc:\n",
    "            print(\n",
    "                f\"Using MCMC with {self.mcmc_steps} steps and {self.mcmc_burnin} burnin\")\n",
    "        else:\n",
    "            print(f\"Using Variational Inference with {self.vi_steps} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Optimizer Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _setup_tf_function_caching(self):\n",
    "    \"\"\"\n",
    "    Configure TensorFlow to reduce function retracing.\n",
    "\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set experimental_relax_shapes=True to reduce retracing due to shape changes\n",
    "        tf.config.optimizer.set_experimental_options({\n",
    "            'layout_optimizer': True,\n",
    "            'constant_folding': True,\n",
    "            'shape_optimization': True,\n",
    "            'remapping': True\n",
    "        })\n",
    "        \n",
    "        # Set environment variable for TF function inlining\n",
    "        os.environ['TF_FUNCTION_JIT_COMPILE_DEFAULT'] = '1'\n",
    "        \n",
    "        # Set up TF memory growth to prevent OOM errors\n",
    "        gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            try:\n",
    "                for gpu in gpus:\n",
    "                    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error setting memory growth: {e}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up TensorFlow optimizations: {e}\")\n",
    "\n",
    "def _create_optimizer(self):\n",
    "    \"\"\"\n",
    "    Create an enhanced optimizer with adaptive learning rate scheduling\n",
    "    specifically tuned for cryptocurrency price prediction.\n",
    "\n",
    "    :return: TensorFlow optimizer instance\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get initial learning rate with fallback\n",
    "        initial_lr = 0.05\n",
    "        if hasattr(self, 'config') and self.config is not None:\n",
    "            service_config = self.config.get(self.service_name, {})\n",
    "            model_config = service_config.get('model', {}).get('instant', {})\n",
    "            if 'learning_rate' in model_config:\n",
    "                initial_lr = model_config['learning_rate']\n",
    "        \n",
    "        # Create a learning rate schedule that adapts to cryptocurrency price volatility\n",
    "        # Implement a custom learning rate scheduler with:\n",
    "        # 1. Warm-up phase to prevent early convergence to poor solutions\n",
    "        # 2. Step decay to reduce learning rate over time\n",
    "        # 3. Minimum learning rate to maintain adaptability\n",
    "        \n",
    "        # Define learning rate schedule parameters\n",
    "        warmup_steps = 30\n",
    "        decay_steps = 50\n",
    "        decay_rate = 0.85\n",
    "        min_learning_rate = 0.001\n",
    "        \n",
    "        # Implement learning rate schedule using TensorFlow's functionality\n",
    "        @tf.function\n",
    "        def lr_schedule(step):\n",
    "            # Convert to float32 for calculations\n",
    "            step_f = tf.cast(step, tf.float32)\n",
    "            warmup_steps_f = tf.constant(warmup_steps, dtype=tf.float32)\n",
    "            \n",
    "            # Warmup phase: linear increase\n",
    "            warmup_factor = tf.minimum(1.0, step_f / warmup_steps_f)\n",
    "            \n",
    "            # Decay phase: exponential decay with step function\n",
    "            decay_factor = decay_rate ** tf.floor(step_f / decay_steps)\n",
    "            \n",
    "            # Combine warmup and decay\n",
    "            lr = initial_lr * warmup_factor * decay_factor\n",
    "            \n",
    "            # Ensure we don't go below minimum learning rate\n",
    "            return tf.maximum(lr, min_learning_rate)\n",
    "            \n",
    "        # Choose optimizer based on dataset size and characteristics\n",
    "        # For cryptocurrency data:\n",
    "        # - Adam works well for general cases\n",
    "        # - RMSprop can be better for high volatility\n",
    "        # - Adagrad/Adadelta can work well with sparse updates\n",
    "        \n",
    "        # Evaluate data characteristics to select optimizer\n",
    "        if hasattr(self, 'observed_time_series') and self.observed_time_series is not None:\n",
    "            data_length = len(self.observed_time_series)\n",
    "            \n",
    "            # For very large datasets, use Adam with weight decay\n",
    "            if data_length > 1000:\n",
    "                print(\"Using AdamW optimizer for large dataset\")\n",
    "                optimizer = tf.keras.optimizers.legacy.Adam(\n",
    "                    learning_rate=lr_schedule,\n",
    "                    beta_1=0.9,  # Default momentum\n",
    "                    beta_2=0.999,  # Default second moment\n",
    "                    epsilon=1e-7,  # Prevent division by zero\n",
    "                    amsgrad=True  # Use AMSGrad variant for better convergence\n",
    "                )\n",
    "            # For medium datasets with high volatility, use RMSprop\n",
    "            elif data_length > 100:\n",
    "                # For cryptocurrency, RMSprop adapts well to changing gradients\n",
    "                print(\"Using RMSprop optimizer for medium dataset\")\n",
    "                optimizer = tf.keras.optimizers.legacy.RMSprop(\n",
    "                    learning_rate=lr_schedule,\n",
    "                    rho=0.9,  # Decay rate for moving average\n",
    "                    momentum=0.0,  # No momentum for faster adaptation\n",
    "                    epsilon=1e-7,  # Numerical stability\n",
    "                    centered=True  # Center the gradient variance for better performance\n",
    "                )\n",
    "            # For small datasets, use more aggressive learning\n",
    "            else:\n",
    "                print(\"Using Adam optimizer with higher learning rate for small dataset\")\n",
    "                # Higher learning rate for small datasets to converge faster\n",
    "                optimizer = tf.keras.optimizers.legacy.Adam(\n",
    "                    learning_rate=lambda step: tf.maximum(initial_lr * 1.5 * decay_rate ** (step // 30), min_learning_rate),\n",
    "                    beta_1=0.9,  # Default momentum\n",
    "                    beta_2=0.99,  # Slightly lower than default for more adaptivity\n",
    "                    epsilon=1e-6  # Slightly higher epsilon for stability\n",
    "                )\n",
    "        else:\n",
    "            # Default optimizer if no data characteristics available\n",
    "            print(\"Using default Adam optimizer\")\n",
    "            optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=initial_lr)\n",
    "        \n",
    "        # Configure optimizer for mixed precision if available\n",
    "        try:\n",
    "            # Try to use mixed precision for better performance\n",
    "            if tf.config.list_physical_devices('GPU'):\n",
    "                print(\"Configuring optimizer for mixed precision on GPU\")\n",
    "                optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n",
    "        except Exception as e:\n",
    "            print(f\"Mixed precision configuration not available: {e}\")\n",
    "        \n",
    "        return optimizer\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating optimizer: {e}. Using default Adam optimizer.\")\n",
    "        return tf.keras.optimizers.legacy.Adam(learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing and Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(self, data: Any) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Preprocess time series data with enhanced technical indicators specifically tuned \n",
    "    for cryptocurrency markets.\n",
    "\n",
    "    :param data: Input time series data\n",
    "    :return: Preprocessed data as tf.Tensor\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert input to numpy array if needed\n",
    "        if isinstance(data, tf.Tensor):\n",
    "            data = data.numpy()\n",
    "\n",
    "        if len(data.shape) == 0:\n",
    "            data = np.array([data])\n",
    "\n",
    "        # Create pandas Series for easier manipulation\n",
    "        series = pd.Series(data)\n",
    "        \n",
    "        # Check data quality and print stats\n",
    "        print(f\"Preprocessing data: {len(series)} points, min={series.min():.2f}, max={series.max():.2f}\")\n",
    "        \n",
    "        # Special preprocessing for cryptocurrency price data\n",
    "        # 1. Enhanced outlier detection using Multiple methods\n",
    "        \n",
    "        # Method 1: Modified Z-Score (more robust than standard Z-score)\n",
    "        median_val = series.median()\n",
    "        mad = np.median(np.abs(series - median_val))\n",
    "        modified_z_scores = 0.6745 * (series - median_val) / (mad + 1e-8)\n",
    "        z_outliers = np.where(np.abs(modified_z_scores) > 3.5)[0]\n",
    "        \n",
    "        # Method 2: Interquartile Range (IQR) - good for skewed distributions like crypto\n",
    "        Q1 = series.quantile(0.25)\n",
    "        Q3 = series.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        iqr_lower = Q1 - 1.8 * IQR  # Slightly more aggressive than standard 1.5\n",
    "        iqr_upper = Q3 + 1.8 * IQR\n",
    "        iqr_outliers = np.where((series < iqr_lower) | (series > iqr_upper))[0]\n",
    "        \n",
    "        # Method 3: Percentage change outliers (specific to crypto volatility)\n",
    "        pct_changes = series.pct_change().fillna(0)\n",
    "        # Find sudden jumps/drops >3% (commonly seen in crypto markets)\n",
    "        pct_outliers = np.where(np.abs(pct_changes) > 0.03)[0]\n",
    "        \n",
    "        # Combine outliers from all methods, but with a consensus approach\n",
    "        # Only flag as outlier if detected by at least 2 methods\n",
    "        all_outliers = list(z_outliers) + list(iqr_outliers) + list(pct_outliers)\n",
    "        outlier_counts = {}\n",
    "        for idx in all_outliers:\n",
    "            outlier_counts[idx] = outlier_counts.get(idx, 0) + 1\n",
    "            \n",
    "        # Get indices with at least 2 detections\n",
    "        consensus_outliers = [idx for idx, count in outlier_counts.items() if count >= 2]\n",
    "        outlier_indices = sorted(consensus_outliers)\n",
    "\n",
    "        if len(outlier_indices) > 0:\n",
    "            print(\n",
    "                f\"Found {len(outlier_indices)} consensus outliers using multiple detection methods\")\n",
    "            for idx in outlier_indices:\n",
    "                # Use exponential weighted average with local context for replacement\n",
    "                # This preserves more of the trend information than simple median\n",
    "                window_size = 10\n",
    "                start_idx = max(0, idx - window_size)\n",
    "                end_idx = min(len(series), idx + window_size + 1)\n",
    "                local_values = series.iloc[start_idx:end_idx].copy()\n",
    "                \n",
    "                # Remove the outlier itself from local values\n",
    "                if idx >= start_idx and idx < end_idx:\n",
    "                    local_values = local_values.drop(local_values.index[idx - start_idx])\n",
    "                    \n",
    "                if not local_values.empty:\n",
    "                    # Enhanced replacement strategy: weighted average of nearby points\n",
    "                    # with exponential decay for distance\n",
    "                    if len(local_values) >= 3:\n",
    "                        # Calculate distances from the outlier point\n",
    "                        distances = np.abs(np.array(local_values.index) - idx)\n",
    "                        # Exponential weights based on distance\n",
    "                        weights = np.exp(-0.3 * distances)\n",
    "                        # Normalize weights\n",
    "                        weights = weights / np.sum(weights)\n",
    "                        # Weighted average\n",
    "                        replacement = np.sum(local_values.values * weights)\n",
    "                    else:\n",
    "                        # Simple mean for very small local context\n",
    "                        replacement = local_values.mean()\n",
    "                        \n",
    "                    series.iloc[idx] = replacement\n",
    "                    print(f\"  Replaced outlier at index {idx} (value: {data[idx]:.2f}) with {replacement:.2f}\")\n",
    "\n",
    "        # Add enhanced technical indicators specific to cryptocurrency markets\n",
    "        if self.use_technical_indicators and len(series) >= self.long_ma_window:\n",
    "            df = pd.DataFrame({'price': series})\n",
    "\n",
    "            # 1. Enhanced moving averages with crypto-specific windows\n",
    "            # Short-term windows for capturing rapid price movements\n",
    "            for window in [3, 5, 8, 13]:  # Fibonacci sequence for crypto markets\n",
    "                df[f'ma_{window}'] = series.rolling(window=window).mean()\n",
    "                # Exponential MA gives more weight to recent prices\n",
    "                df[f'ema_{window}'] = series.ewm(span=window, adjust=False).mean()\n",
    "\n",
    "            # 2. Volatility indicators (especially important for crypto)\n",
    "            # ATR-inspired volatility measure\n",
    "            for window in [5, 8, 13, 21]:\n",
    "                price_diffs = np.abs(series.diff())\n",
    "                df[f'volatility_{window}'] = price_diffs.rolling(window=window).mean()\n",
    "            \n",
    "            # 3. Crypto-specific momentum indicators\n",
    "            for period in [3, 5, 8, 13]:\n",
    "                # ROC (Rate of Change) - critical for crypto momentum trading\n",
    "                df[f'roc_{period}'] = series.pct_change(periods=period) * 100\n",
    "                # Price Velocity - captures speed of price movement\n",
    "                df[f'velocity_{period}'] = series.diff(periods=period) / period\n",
    "            \n",
    "            # 4. Bollinger Bands - popular for crypto trading\n",
    "            for window in [13, 21]:\n",
    "                ma = df['price'].rolling(window=window).mean()\n",
    "                std = df['price'].rolling(window=window).std()\n",
    "                df[f'bb_upper_{window}'] = ma + (2 * std)\n",
    "                df[f'bb_lower_{window}'] = ma - (2 * std)\n",
    "                # BB width indicates volatility\n",
    "                df[f'bb_width_{window}'] = (df[f'bb_upper_{window}'] - df[f'bb_lower_{window}']) / ma\n",
    "\n",
    "            # 5. RSI - crucial for crypto markets\n",
    "            for period in [7, 14]:\n",
    "                delta = series.diff()\n",
    "                gain = delta.clip(lower=0)\n",
    "                loss = -delta.clip(upper=0)\n",
    "                avg_gain = gain.rolling(window=period).mean()\n",
    "                avg_loss = loss.rolling(window=period).mean()\n",
    "                # Avoid division by zero\n",
    "                rs = avg_gain / avg_loss.replace(0, np.finfo(float).eps)\n",
    "                df[f'rsi_{period}'] = 100 - (100 / (1 + rs))\n",
    "            \n",
    "            # 6. MACD for crypto trends\n",
    "            # Standard MACD\n",
    "            fast_ema = series.ewm(span=12, adjust=False).mean()\n",
    "            slow_ema = series.ewm(span=26, adjust=False).mean()\n",
    "            macd = fast_ema - slow_ema\n",
    "            signal = macd.ewm(span=9, adjust=False).mean()\n",
    "            df['macd'] = macd\n",
    "            df['macd_signal'] = signal\n",
    "            df['macd_hist'] = macd - signal\n",
    "            \n",
    "            # 7. Fractal indicators for crypto (simplified)\n",
    "            if len(series) >= 5:\n",
    "                highs = series.rolling(window=5, center=True).max()\n",
    "                lows = series.rolling(window=5, center=True).min()\n",
    "                df['fractal_high'] = (highs == series)\n",
    "                df['fractal_low'] = (lows == series)\n",
    "\n",
    "            # Fill NaN values with appropriate method\n",
    "            # First forward fill, then backward fill for any remaining NaNs\n",
    "            df = df.ffill().bfill()\n",
    "\n",
    "            # Normalize features to similar scale using robust scaling\n",
    "            # This works better than standard scaling for outlier-prone crypto data\n",
    "            for col in df.columns:\n",
    "                if col != 'price':\n",
    "                    median = df[col].median()\n",
    "                    iqr = df[col].quantile(0.75) - df[col].quantile(0.25)\n",
    "                    if iqr > 0:\n",
    "                        df[col] = (df[col] - median) / (iqr + 1e-8)\n",
    "                    else:\n",
    "                        df[col] = (df[col] - median) / (df[col].std() + 1e-8)\n",
    "\n",
    "            # Store preprocessed data\n",
    "            self.preprocessed_data = df\n",
    "            \n",
    "            print(f\"Generated {len(df.columns)-1} technical indicators for cryptocurrency analysis\")\n",
    "\n",
    "            # Return tensor for model - just the price series\n",
    "            return tf.convert_to_tensor(series.values, dtype=tf.float64)\n",
    "\n",
    "        # Store preprocessed data\n",
    "        self.preprocessed_data = series\n",
    "\n",
    "        # Return tensor for model\n",
    "        return tf.convert_to_tensor(series.values, dtype=tf.float64)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"Error in data preprocessing: {e}\\n{traceback.format_exc()}\")\n",
    "        return tf.convert_to_tensor(data, dtype=tf.float64)\n",
    "\n",
    "def build_model(self, observed_time_series: tf.Tensor) -> Any:\n",
    "    \"\"\"\n",
    "    Build an enhanced structural time series model with multiple components\n",
    "    to better capture price dynamics, especially during rapid changes.\n",
    "\n",
    "    :param observed_time_series: Tensor of observed Bitcoin prices\n",
    "    :return: TFP structural time series model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert input to float64 tensor\n",
    "        observed_time_series = tf.convert_to_tensor(\n",
    "            observed_time_series, dtype=tf.float64)\n",
    "\n",
    "        # Create components list\n",
    "        components = []\n",
    "\n",
    "        # Use much tighter priors for Bitcoin price modeling\n",
    "        # Lower volatility in level scale for more stable predictions\n",
    "        level_scale_prior = tfd.LogNormal(\n",
    "            loc=tf.constant(-5., dtype=tf.float64),  # Much tighter prior for stability\n",
    "            scale=tf.constant(0.3, dtype=tf.float64)  # Narrower distribution\n",
    "        )\n",
    "\n",
    "        # More appropriate slope scale for cryptocurrency dynamics\n",
    "        slope_scale_prior = tfd.LogNormal(\n",
    "            loc=tf.constant(-4., dtype=tf.float64),\n",
    "            scale=tf.constant(0.5, dtype=tf.float64)\n",
    "        )\n",
    "\n",
    "        # Initialize level at the first observation with smaller variance\n",
    "        initial_level_prior = tfd.Normal(\n",
    "            loc=observed_time_series[0],\n",
    "            scale=tf.constant(100., dtype=tf.float64)  # Reduced from 1000 to 100\n",
    "        )\n",
    "\n",
    "        # Allow for non-zero initial slope to capture trends immediately\n",
    "        if len(observed_time_series) >= 3:\n",
    "            # Calculate initial slope from first few observations with exponential weighting\n",
    "            # This puts more emphasis on the most recent trend\n",
    "            if len(observed_time_series) >= 10:\n",
    "                # Use more points for a more stable initial slope\n",
    "                weights = np.exp(np.linspace(0, 1, 10))\n",
    "                weights = weights / np.sum(weights)\n",
    "                diffs = np.diff(observed_time_series[:10].numpy())\n",
    "                initial_slope = np.sum(diffs * weights[:len(diffs)])\n",
    "            else:\n",
    "                # Simple approach for very short series\n",
    "                initial_slope = (observed_time_series[2] - observed_time_series[0]) / 2.0\n",
    "            \n",
    "            initial_slope_prior = tfd.Normal(\n",
    "                loc=tf.constant(initial_slope, dtype=tf.float64),\n",
    "                scale=tf.constant(50., dtype=tf.float64)  # Reduced from 100 to 50\n",
    "            )\n",
    "        else:\n",
    "            initial_slope_prior = tfd.Normal(\n",
    "                loc=tf.constant(0., dtype=tf.float64),\n",
    "                scale=tf.constant(50., dtype=tf.float64)\n",
    "            )\n",
    "        \n",
    "        # Local linear trend component with explicit float64 priors\n",
    "        local_linear_trend = tfs.LocalLinearTrend(\n",
    "            observed_time_series=observed_time_series,\n",
    "            level_scale_prior=level_scale_prior,\n",
    "            slope_scale_prior=slope_scale_prior,\n",
    "            initial_level_prior=initial_level_prior,\n",
    "            initial_slope_prior=initial_slope_prior,\n",
    "            name='local_linear_trend'\n",
    "        )\n",
    "\n",
    "        # First add the local linear trend component\n",
    "        components.append(local_linear_trend)\n",
    "        \n",
    "        # Create seasonal prior with explicit float64 dtype and tighter constraints\n",
    "        drift_scale_prior = tfd.LogNormal(\n",
    "            loc=tf.constant(-4., dtype=tf.float64),  # Tighter prior\n",
    "            scale=tf.constant(0.3, dtype=tf.float64)  # Reduced variability\n",
    "        )\n",
    "        \n",
    "        # Add enhanced seasonality components - specifically tuned for crypto markets\n",
    "        # Add both daily and weekly seasonality for crypto markets\n",
    "        \n",
    "        # 24-hour cycle for intraday patterns (if data frequency permits)\n",
    "        if self.num_timesteps >= 48:  # At least two full cycles recommended\n",
    "            daily_seasonal = tfs.Seasonal(\n",
    "                num_seasons=24,\n",
    "                observed_time_series=observed_time_series,\n",
    "                drift_scale_prior=drift_scale_prior,\n",
    "                name='daily_seasonal'\n",
    "            )\n",
    "            components.append(daily_seasonal)\n",
    "        \n",
    "        # 7-day cycle for weekly patterns (if enough data available)\n",
    "        if self.num_timesteps >= 168:  # 7 days Ã 24 hours\n",
    "            weekly_seasonal = tfs.Seasonal(\n",
    "                num_seasons=7,\n",
    "                observed_time_series=observed_time_series,\n",
    "                drift_scale_prior=drift_scale_prior,\n",
    "                name='weekly_seasonal'\n",
    "            )\n",
    "            components.append(weekly_seasonal)\n",
    "        \n",
    "        # Standard seasonal component based on frequency pattern\n",
    "        seasonal = tfs.Seasonal(\n",
    "            num_seasons=self.num_seasons,\n",
    "            observed_time_series=observed_time_series,\n",
    "            drift_scale_prior=drift_scale_prior,\n",
    "            name='seasonal'\n",
    "        )\n",
    "        components.append(seasonal)\n",
    "\n",
    "        # Enhanced autoregressive component with higher order for better short-term predictions\n",
    "        # Use AR(5) for cryptocurrency data which has complex short-term dynamics\n",
    "        ar_order = 5\n",
    "\n",
    "        # Only use higher-order AR if we have enough data\n",
    "        if len(observed_time_series) > ar_order * 3:\n",
    "            # Use a semi-local parameterization for more stability\n",
    "            autoregressive = tfs.Autoregressive(\n",
    "                order=ar_order,\n",
    "                observed_time_series=observed_time_series,\n",
    "                name='autoregressive'\n",
    "            )\n",
    "            components.append(autoregressive)\n",
    "        else:\n",
    "            # Fall back to AR(2) for short time series\n",
    "            autoregressive = tfs.Autoregressive(\n",
    "                order=2,\n",
    "                observed_time_series=observed_time_series,\n",
    "                name='autoregressive'\n",
    "            )\n",
    "            components.append(autoregressive)\n",
    "        \n",
    "        # Add a SemiLocalLinearTrend for better handling of cryptocurrency volatility\n",
    "        if len(observed_time_series) > 20:  # Only if we have enough data\n",
    "            try:\n",
    "                semi_local_linear_trend = tfs.SemiLocalLinearTrend(\n",
    "                    observed_time_series=observed_time_series,\n",
    "                    name='semi_local_linear_trend'\n",
    "                )\n",
    "                components.append(semi_local_linear_trend)\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Could not add SemiLocalLinearTrend: {e}\")\n",
    "\n",
    "        # Verify we have valid components before creating the model\n",
    "        if not components:\n",
    "            print(\"Error: No valid components to build model\")\n",
    "            return None\n",
    "\n",
    "        # Combine components with Sum\n",
    "        model = tfs.Sum(\n",
    "            components=components,\n",
    "            observed_time_series=observed_time_series\n",
    "        )\n",
    "        \n",
    "        # Clear any old model resources\n",
    "        if self.model is not None:\n",
    "            del self.model\n",
    "            gc.collect()\n",
    "\n",
    "        self.model = model\n",
    "        self.model_version += 1\n",
    "\n",
    "        print(\n",
    "            f\"Built enhanced model v{self.model_version} with {len(components)} components\")\n",
    "        return model\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error building model: {e}\\n{traceback.format_exc()}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fitting and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, observed_time_series: Any, num_variational_steps: Optional[int] = None) -> Any:\n",
    "    \"\"\"\n",
    "    Fit the model to the observed time series.\n",
    "\n",
    "    :param observed_time_series: Tensor of observed Bitcoin prices\n",
    "    :param num_variational_steps: Number of optimization steps (optional)\n",
    "    :return: Fitted model or posterior\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use provided steps or fall back to config\n",
    "        if num_variational_steps is None:\n",
    "            num_variational_steps = self.vi_steps\n",
    "\n",
    "        # Preprocess data first\n",
    "        processed_data = self.preprocess_data(observed_time_series)\n",
    "\n",
    "        # Build a new model or rebuild if needed\n",
    "        if self.model is None:\n",
    "            self.build_model(processed_data)\n",
    "    \n",
    "        # Convert to tensor and ensure float64\n",
    "        self.observed_time_series = processed_data\n",
    "\n",
    "        # Choose between MCMC or Variational Inference\n",
    "        # Only use MCMC with sufficient data\n",
    "        if self.use_mcmc and len(processed_data) > 10:\n",
    "            return self._fit_mcmc()\n",
    "        else:\n",
    "            return self._fit_variational_inference(num_variational_steps)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting model: {e}\\n{traceback.format_exc()}\")\n",
    "        return None\n",
    "\n",
    "def _fit_variational_inference(self, num_steps: int) -> Any:\n",
    "    \"\"\"\n",
    "    Fit the model using variational inference with enhanced optimization strategies.\n",
    "\n",
    "    :param num_steps: Number of optimization steps\n",
    "    :return: Surrogate posterior\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if model is valid\n",
    "        if self.model is None:\n",
    "            print(\"Error: Cannot fit variational inference - model is None\")\n",
    "            return None\n",
    "\n",
    "        # Clear old TF variables by creating a new surrogate posterior\n",
    "        # Build surrogate posterior - this creates new TF variables\n",
    "        try:\n",
    "            # Use factored surrogate posterior with tailored initialization\n",
    "            surrogate = tfs.build_factored_surrogate_posterior(\n",
    "                model=self.model,\n",
    "                initial_loc_fn=lambda *args: tfd.Normal(loc=0.0, scale=0.01).sample(*args)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error building surrogate posterior: {e}\")\n",
    "            return None\n",
    "\n",
    "        # Create a new optimizer for each fit to prevent variable sharing issues\n",
    "        optimizer = self._create_optimizer()\n",
    "        \n",
    "        # Define joint log probability function with numerical stability improvements\n",
    "        @tf.function(experimental_relax_shapes=True, reduce_retracing=True)\n",
    "        def target_log_prob_fn(**params):\n",
    "            # Add small epsilon to potentially zero values to avoid numerical issues\n",
    "            safe_params = {}\n",
    "            for param_name, param_value in params.items():\n",
    "                if 'scale' in param_name:\n",
    "                    # Add small epsilon to scale parameters to ensure positive values\n",
    "                    safe_params[param_name] = param_value + 1e-8\n",
    "                else:\n",
    "                    safe_params[param_name] = param_value\n",
    "            \n",
    "            return self.model.joint_distribution(\n",
    "                observed_time_series=self.observed_time_series\n",
    "            ).log_prob(**params)\n",
    "        \n",
    "        # Implement early stopping to prevent overfitting\n",
    "        patience = 10  # Number of steps to wait after validation improvement\n",
    "        min_delta = 0.001  # Minimum change to qualify as improvement\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        early_stopping = False\n",
    "        \n",
    "        # Dynamically adjust steps for dataset size\n",
    "        # Use fewer steps for smaller datasets to speed up computation\n",
    "        actual_steps = num_steps\n",
    "        if len(self.observed_time_series) < 30:\n",
    "            # For small datasets, fewer steps are needed\n",
    "            actual_steps = max(50, int(num_steps * 0.6))\n",
    "            print(f\"Small dataset detected, using reduced VI steps: {actual_steps}\")\n",
    "        elif len(self.observed_time_series) > 100:\n",
    "            # For large datasets, ensure sufficient steps for convergence\n",
    "            actual_steps = min(200, int(num_steps * 1.2))\n",
    "            print(f\"Large dataset detected, using increased VI steps: {actual_steps}\")\n",
    "        else:\n",
    "            actual_steps = num_steps\n",
    "            \n",
    "        # Implement multi-start optimization to avoid local minima\n",
    "        # Try 3 different initializations and pick the best\n",
    "        best_surrogate = None\n",
    "        best_loss_value = float('inf')\n",
    "        \n",
    "        for start_idx in range(3):\n",
    "            # Reset the surrogate for each start\n",
    "            if start_idx > 0:\n",
    "                try:\n",
    "                    surrogate = tfs.build_factored_surrogate_posterior(\n",
    "                        model=self.model,\n",
    "                        initial_loc_fn=lambda *args: tfd.Normal(loc=0.0, scale=0.01 * (start_idx + 1)).sample(*args)\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Error rebuilding surrogate posterior for start {start_idx}: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                # Create a fresh optimizer for each start\n",
    "                optimizer = self._create_optimizer()\n",
    "                \n",
    "            # Custom training loop with early stopping\n",
    "            @tf.function(experimental_relax_shapes=True)\n",
    "            def run_vi_step(step):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    loss = -surrogate.variational_loss(target_log_prob_fn)\n",
    "                grads = tape.gradient(loss, surrogate.trainable_variables)\n",
    "                \n",
    "                # Gradient clipping to prevent exploding gradients\n",
    "                grads, _ = tf.clip_by_global_norm(grads, 5.0)\n",
    "                \n",
    "                optimizer.apply_gradients(zip(grads, surrogate.trainable_variables))\n",
    "                return loss\n",
    "            \n",
    "            # Run optimization with early stopping\n",
    "            losses = []\n",
    "            for step in range(actual_steps):\n",
    "                loss_value = run_vi_step(tf.constant(step, dtype=tf.int32))\n",
    "                losses.append(loss_value)\n",
    "                \n",
    "                # Check for early stopping every few steps\n",
    "                if step % 10 == 0 and step > 0:\n",
    "                    current_loss = loss_value.numpy()\n",
    "                    \n",
    "                    if current_loss < best_loss - min_delta:\n",
    "                        best_loss = current_loss\n",
    "                        patience_counter = 0\n",
    "                    else:\n",
    "                        patience_counter += 1\n",
    "                        \n",
    "                    if patience_counter >= patience:\n",
    "                        print(f\"Early stopping triggered at step {step}\")\n",
    "                        early_stopping = True\n",
    "                        break\n",
    "            \n",
    "            # Track the best surrogate across different starts\n",
    "            final_loss = losses[-1].numpy()\n",
    "            if final_loss < best_loss_value:\n",
    "                best_loss_value = final_loss\n",
    "                best_surrogate = surrogate\n",
    "                print(f\"New best surrogate from start {start_idx} with loss {final_loss:.4f}\")\n",
    "                \n",
    "            # Break if we've found a good solution\n",
    "            if early_stopping and best_loss_value < -1000:\n",
    "                print(f\"Good solution found early, skipping remaining starts\")\n",
    "                break\n",
    "        \n",
    "        # Use the best surrogate found\n",
    "        if best_surrogate is not None:\n",
    "            surrogate = best_surrogate\n",
    "            print(f\"Using best surrogate with loss {best_loss_value:.4f}\")\n",
    "        \n",
    "        # Explicitly clear the old posterior to release memory\n",
    "        if self.posterior is not None:\n",
    "            del self.posterior\n",
    "            gc.collect()\n",
    "    \n",
    "        self.posterior = surrogate\n",
    "\n",
    "        # Log the final loss for monitoring convergence\n",
    "        if len(losses) > 0:\n",
    "            print(f\"Final VI loss: {losses[-1].numpy()}\")\n",
    "\n",
    "        return surrogate\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"Error in variational inference: {e}\\n{traceback.format_exc()}\")\n",
    "        return None\n",
    "\n",
    "def _fit_mcmc(self) -> Any:\n",
    "    \"\"\"\n",
    "    Fit the model using MCMC for more accurate inference.\n",
    "\n",
    "    :return: Posterior from MCMC\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Define joint log probability function\n",
    "        def target_log_prob_fn(**params):\n",
    "            return self.model.joint_distribution(\n",
    "                observed_time_series=self.observed_time_series\n",
    "            ).log_prob(**params)\n",
    "\n",
    "        # Set the step size to be adapting during burnin\n",
    "        step_size = tf.Variable(0.01, dtype=tf.float64)\n",
    "\n",
    "        # Create transition kernel\n",
    "        hmc_kernel = tfp.mcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn=target_log_prob_fn,\n",
    "            step_size=step_size,\n",
    "            num_leapfrog_steps=3\n",
    "        )\n",
    "\n",
    "        # Adapt step size during burnin\n",
    "        adaptive_kernel = tfp.mcmc.SimpleStepSizeAdaptation(\n",
    "            inner_kernel=hmc_kernel,\n",
    "            num_adaptation_steps=int(self.mcmc_burnin * 0.8),\n",
    "            target_accept_prob=tf.constant(0.75, dtype=tf.float64)\n",
    "        )\n",
    "\n",
    "        # Initialize MCMC state from the model priors\n",
    "        init_state = [tf.random.normal([])\n",
    "                        for _ in range(len(self.model.parameters))]\n",
    "\n",
    "        # Run the MCMC chain\n",
    "        @tf.function(autograph=False)\n",
    "        def run_chain():\n",
    "            samples, _ = tfp.mcmc.sample_chain(\n",
    "                num_results=self.mcmc_steps,\n",
    "                num_burnin_steps=self.mcmc_burnin,\n",
    "                current_state=init_state,\n",
    "                kernel=adaptive_kernel,\n",
    "                trace_fn=lambda _, pkr: pkr.inner_results.is_accepted\n",
    "            )\n",
    "            return samples\n",
    "\n",
    "        print(\n",
    "            f\"Starting MCMC with {self.mcmc_steps} steps and {self.mcmc_burnin} burnin...\")\n",
    "        samples = run_chain()\n",
    "        print(\"MCMC sampling completed\")\n",
    "\n",
    "        # Create a callable posterior from MCMC samples\n",
    "        def sample_fn(sample_shape=(), seed=None):\n",
    "            \"\"\"Sample from the MCMC results.\"\"\"\n",
    "            idx = tf.random.uniform(\n",
    "                shape=sample_shape,\n",
    "                minval=0,\n",
    "                maxval=self.mcmc_steps,\n",
    "                dtype=tf.int32,\n",
    "                seed=seed\n",
    "            )\n",
    "            return [tf.gather(chain, idx) for chain in samples]\n",
    "\n",
    "        # Create a posterior object with the sample function\n",
    "        class MCMCPosterior:\n",
    "            def __init__(self, sample_function):\n",
    "                self.sample_function = sample_function\n",
    "\n",
    "            def sample(self, sample_shape=(), seed=None):\n",
    "                return self.sample_function(sample_shape, seed)\n",
    "\n",
    "        # Create and store the posterior\n",
    "        self.posterior = MCMCPosterior(sample_fn)\n",
    "        return self.posterior\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in MCMC inference: {e}\\n{traceback.format_exc()}\")\n",
    "        # Fall back to variational inference if MCMC fails\n",
    "        print(\"Falling back to variational inference\")\n",
    "        return self._fit_variational_inference(self.vi_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Forecasting and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast(self, num_steps: int = 1) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Generate forecasts with uncertainty intervals using ensemble techniques for higher accuracy.\n",
    "\n",
    "    :param num_steps: Number of steps ahead to forecast (default: 1)\n",
    "    :return: Tuple of (mean prediction, lower bound, upper bound)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if model and posterior exist\n",
    "        if self.model is None:\n",
    "            print(\"[{}] Warning: Model is None, using last forecast as fallback\".format(\n",
    "                datetime.now().isoformat()\n",
    "            ))\n",
    "            if self.last_forecast is not None:\n",
    "                return self.last_mean, self.last_lower, self.last_upper\n",
    "            return self._fallback_forecast()\n",
    "\n",
    "        if self.posterior is None:\n",
    "            print(\"[{}] Warning: Posterior is None, using last forecast as fallback\".format(\n",
    "                datetime.now().isoformat()\n",
    "            ))\n",
    "            if self.last_forecast is not None:\n",
    "                return self.last_mean, self.last_lower, self.last_upper\n",
    "            return self._fallback_forecast()\n",
    "\n",
    "        print(\n",
    "            f\"[{datetime.now().isoformat()}] Making forecast with TFP model v{self.model_version}\")\n",
    "\n",
    "        # Use more samples for more accurate prediction distribution\n",
    "        increased_samples = min(100, self.num_samples * 2)  # Double samples but cap at 100\n",
    "        \n",
    "        # Create ensemble of forecasts from multiple sampling runs\n",
    "        ensemble_predictions = []\n",
    "        ensemble_scales = []\n",
    "        \n",
    "        # Make 3 independent forecast runs and ensemble them\n",
    "        for ensemble_run in range(3):\n",
    "            # Generate samples from the posterior and forecast using cached function\n",
    "            forecast_dist = self._generate_forecast(num_steps)\n",
    "            \n",
    "            # Extract forecast samples\n",
    "            mean_forecast, scale_forecast = self._extract_forecast_stats(forecast_dist)\n",
    "            \n",
    "            # Add to ensemble\n",
    "            ensemble_predictions.append(mean_forecast)\n",
    "            ensemble_scales.append(scale_forecast)\n",
    "        \n",
    "        # Compute ensemble prediction (weighted by inverse of scale)\n",
    "        weights = [1.0 / (s + 1e-6) for s in ensemble_scales]  # Add epsilon to avoid division by zero\n",
    "        total_weight = sum(weights)\n",
    "        normalized_weights = [w / total_weight for w in weights]\n",
    "        \n",
    "        # Weighted average for the final prediction\n",
    "        mean_forecast_value = sum(p * w for p, w in zip(ensemble_predictions, normalized_weights))\n",
    "        \n",
    "        # Take the most conservative (largest) scale for uncertainty bounds\n",
    "        scale_forecast_value = max(ensemble_scales)\n",
    "        \n",
    "        # Extract scalars using utility function\n",
    "        mean_forecast_value = extract_scalar_from_prediction(mean_forecast_value)\n",
    "        scale_forecast_value = extract_scalar_from_prediction(scale_forecast_value)\n",
    "\n",
    "        # Calculate prediction intervals with wider bounds for cryptocurrency\n",
    "        # Use 99% confidence for crypto instead of 95% (2.58 vs 1.96)\n",
    "        lower = mean_forecast_value - 2.58 * scale_forecast_value\n",
    "        upper = mean_forecast_value + 2.58 * scale_forecast_value\n",
    "        \n",
    "        # Implement sanity checks for crypto predictions\n",
    "        # Ensure prediction is within reasonable bounds (e.g., not too far from current price)\n",
    "        last_observed = extract_scalar_from_prediction(self.observed_time_series[-1])\n",
    "        max_allowed_change = 0.05 * last_observed  # Max 5% change from last price\n",
    "        \n",
    "        if abs(mean_forecast_value - last_observed) > max_allowed_change:\n",
    "            # Adjust prediction to be closer to last observed price\n",
    "            print(f\"Forecast {mean_forecast_value:.2f} differs too much from last price {last_observed:.2f}. Adjusting.\")\n",
    "            direction = 1 if mean_forecast_value > last_observed else -1\n",
    "            mean_forecast_value = last_observed + direction * max_allowed_change\n",
    "            \n",
    "            # Recalculate bounds with the new mean\n",
    "            lower = mean_forecast_value - 2.58 * scale_forecast_value\n",
    "            upper = mean_forecast_value + 2.58 * scale_forecast_value\n",
    "\n",
    "        # Store for fallback\n",
    "        self.last_forecast = forecast_dist\n",
    "        self.last_mean = mean_forecast_value\n",
    "        self.last_lower = lower\n",
    "        self.last_upper = upper\n",
    "\n",
    "        # Round values for consistency\n",
    "        mean_forecast_value = safe_round(mean_forecast_value, 2)\n",
    "        lower = safe_round(lower, 2)\n",
    "        upper = safe_round(upper, 2)\n",
    "\n",
    "        # Return point forecast and interval\n",
    "        return mean_forecast_value, lower, upper\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in forecast: {e}\\n{traceback.format_exc()}\")\n",
    "        print(\"Using last forecast as fallback\")\n",
    "\n",
    "        # Return last successful forecast if available\n",
    "        if self.last_forecast is not None:\n",
    "            return self.last_mean, self.last_lower, self.last_upper\n",
    "\n",
    "        # Otherwise use fallback method\n",
    "        return self._fallback_forecast()\n",
    "\n",
    "@tf.function(experimental_relax_shapes=True)\n",
    "def _generate_forecast(self, num_steps: int) -> Any:\n",
    "    \"\"\"\n",
    "    Generate forecast distribution with TensorFlow function caching.\n",
    "\n",
    "    :param num_steps: Number of steps ahead to forecast\n",
    "    :return: Forecast distribution\n",
    "    \"\"\"\n",
    "    return tfs.forecast(\n",
    "        model=self.model,\n",
    "        observed_time_series=self.observed_time_series,\n",
    "        parameter_samples=self.posterior.sample(self.num_samples),\n",
    "        num_steps_forecast=num_steps\n",
    "    )\n",
    "\n",
    "def _extract_forecast_stats(self, forecast_dist: Any) -> Tuple[Any, Any]:\n",
    "    \"\"\"\n",
    "    Extract mean and standard deviation from forecast distribution.\n",
    "\n",
    "    :param forecast_dist: Forecast distribution\n",
    "    :return: Tuple of (mean, stddev)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use TensorFlow operations directly when possible\n",
    "        forecast_means = forecast_dist.mean()[0]  # Get first step mean\n",
    "        forecast_scales = forecast_dist.stddev()[0]  # Get first step stddev\n",
    "        return forecast_means, forecast_scales\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting forecast stats: {e}\")\n",
    "        # Fallback to numpy arrays if TensorFlow ops fail\n",
    "        return np.array([0.0]), np.array([0.0])\n",
    "\n",
    "def evaluate_prediction(self, actual_price: float, prediction: float, timestamp: Optional[Any] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate a prediction against the actual price and track errors.\n",
    "\n",
    "    :param actual_price: Actual observed price\n",
    "    :param prediction: Predicted price\n",
    "    :param timestamp: Optional timestamp for the prediction\n",
    "    :return: Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert inputs to scalar values\n",
    "        actual = extract_scalar_from_prediction(actual_price)\n",
    "        pred = extract_scalar_from_prediction(prediction)\n",
    "        \n",
    "        # Calculate absolute error\n",
    "        error = actual - pred\n",
    "        abs_error = abs(error)\n",
    "\n",
    "        # Track recent errors for anomaly detection\n",
    "        self.recent_errors.append(abs_error)\n",
    "        if len(self.recent_errors) > self.max_error_history:\n",
    "            self.recent_errors.pop(0)\n",
    "\n",
    "        # Calculate percentage error\n",
    "        pct_error = (error / actual) * 100 if actual != 0 else float('inf')\n",
    "\n",
    "        # Calculate z-score of current error\n",
    "        z_score = 0\n",
    "        if len(self.recent_errors) > 5:\n",
    "            mean_error = np.mean(self.recent_errors)\n",
    "            mean_error_value = extract_scalar_from_prediction(mean_error)\n",
    "            \n",
    "            std_error = np.std(self.recent_errors) + 1e-8  # Avoid division by zero\n",
    "            std_error_value = extract_scalar_from_prediction(std_error)\n",
    "            \n",
    "            z_score = (abs_error - mean_error_value) / std_error_value\n",
    "\n",
    "        # Detect anomalies\n",
    "        is_anomaly = z_score > self.anomaly_detection_threshold\n",
    "\n",
    "        # Round metrics to 2 decimal places\n",
    "        abs_error = safe_round(abs_error, 2)\n",
    "        pct_error = safe_round(pct_error, 2)\n",
    "        z_score = safe_round(z_score, 2)\n",
    "\n",
    "        return {\n",
    "            'absolute_error': abs_error,\n",
    "            'percentage_error': pct_error,\n",
    "            'z_score': z_score,\n",
    "            'is_anomaly': is_anomaly,\n",
    "            'timestamp': timestamp\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"Error evaluating prediction: {e}\\n{traceback.format_exc()}\")\n",
    "        return {\n",
    "            'absolute_error': float('nan'),\n",
    "            'percentage_error': float('nan'),\n",
    "            'z_score': float('nan'),\n",
    "            'is_anomaly': False,\n",
    "            'timestamp': timestamp\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Update and Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self, new_data_point: Any) -> bool:\n",
    "    \"\"\"\n",
    "    Update the model with new data, using adaptive learning strategies\n",
    "    specifically designed for cryptocurrency price movements.\n",
    "\n",
    "    :param new_data_point: New observation to incorporate\n",
    "    :return: True if update successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check and validate input\n",
    "        if isinstance(new_data_point, (int, float)):\n",
    "            new_data = np.array([new_data_point], dtype=np.float64)\n",
    "        elif isinstance(new_data_point, tf.Tensor):\n",
    "            new_data = new_data_point.numpy()\n",
    "        elif isinstance(new_data_point, np.ndarray):\n",
    "            new_data = new_data_point\n",
    "        elif isinstance(new_data_point, list):\n",
    "            new_data = np.array(new_data_point, dtype=np.float64)\n",
    "        else:\n",
    "            print(f\"Warning: Unsupported data type for update: {type(new_data_point)}\")\n",
    "            return False\n",
    "\n",
    "        # Adaptive update strategy for cryptocurrency price data\n",
    "        # Detect if there's been a significant price change that requires more VI steps\n",
    "        significant_change = False\n",
    "        volatility_increased = False\n",
    "        \n",
    "        # Check if we have historical data to compare with\n",
    "        if self.observed_time_series is not None and len(self.observed_time_series) > 0:\n",
    "            last_price = extract_scalar_from_prediction(self.observed_time_series[-1])\n",
    "            new_price = extract_scalar_from_prediction(new_data[-1])\n",
    "            \n",
    "            # Calculate percentage change\n",
    "            if abs(last_price) > 1e-6:  # Avoid division by zero\n",
    "                pct_change = abs((new_price - last_price) / last_price)\n",
    "                \n",
    "                # For crypto, consider >1% a significant move requiring extra update steps\n",
    "                if pct_change > 0.01:\n",
    "                    significant_change = True\n",
    "                    print(f\"Significant price change detected: {pct_change:.2%}. Using enhanced update.\")\n",
    "            \n",
    "            # Check for increased volatility (using recent standard deviation)\n",
    "            if len(self.observed_time_series) >= 10:\n",
    "                # Get last 10 observations\n",
    "                recent_data = self.observed_time_series[-10:].numpy()\n",
    "                # Add new data point\n",
    "                combined_data = np.append(recent_data, new_data[-1])\n",
    "                \n",
    "                # Calculate standard deviations\n",
    "                recent_std = np.std(recent_data)\n",
    "                new_std = np.std(combined_data)\n",
    "                \n",
    "                # Check if volatility has increased significantly (>20%)\n",
    "                if new_std > recent_std * 1.2:\n",
    "                    volatility_increased = True\n",
    "                    print(f\"Volatility increase detected: {(new_std/recent_std-1)*100:.1f}%. Using enhanced update.\")\n",
    "\n",
    "        # Append new data to observed time series\n",
    "        if self.observed_time_series is None:\n",
    "            self.observed_time_series = tf.convert_to_tensor(new_data, dtype=tf.float64)\n",
    "        else:\n",
    "            # Append the new data, ensuring it's a tensor\n",
    "            new_data_tensor = tf.convert_to_tensor(new_data, dtype=tf.float64)\n",
    "            self.observed_time_series = tf.concat(\n",
    "                [self.observed_time_series, new_data_tensor], axis=0)\n",
    "        \n",
    "        # Limit dataset size to prevent unbounded growth\n",
    "        if len(self.observed_time_series) > self.max_history_size:\n",
    "            # Keep more recent data for crypto (more relevant for prediction)\n",
    "            print(f\"Limiting history to {self.max_history_size} points\")\n",
    "            self.observed_time_series = self.observed_time_series[-self.max_history_size:]\n",
    "\n",
    "        # Store the number of timesteps\n",
    "        self.num_timesteps = len(self.observed_time_series)\n",
    "        \n",
    "        # Apply preprocessing to the new time series\n",
    "        preprocessed_data = self.preprocess_data(self.observed_time_series)\n",
    "        if preprocessed_data is not None:\n",
    "            self.observed_time_series = preprocessed_data\n",
    "        \n",
    "        # Verify that we have sufficient data for model fitting\n",
    "        if len(self.observed_time_series) < self.min_points_req:\n",
    "            print(\n",
    "                f\"Not enough data points ({len(self.observed_time_series)}) for fitting. Need {self.min_points_req}.\")\n",
    "            return False\n",
    "\n",
    "        # Rebuild the model with the updated time series\n",
    "        self.model = self.build_model(self.observed_time_series)\n",
    "        if self.model is None:\n",
    "            print(\"Failed to build model during update\")\n",
    "            return False\n",
    "            \n",
    "        # Determine number of VI steps based on data characteristics\n",
    "        vi_steps = self.num_variational_steps\n",
    "        \n",
    "        # Boost VI steps for significant changes or increased volatility\n",
    "        if significant_change or volatility_increased:\n",
    "            vi_steps = int(vi_steps * 1.5)  # 50% more steps for better adaptation\n",
    "            print(f\"Using increased VI steps: {vi_steps}\")\n",
    "        \n",
    "        # For large datasets, adjust steps to prevent excessive computation\n",
    "        if len(self.observed_time_series) > 100:\n",
    "            # Cap at a maximum based on available computational resources\n",
    "            vi_steps = min(vi_steps, 150)\n",
    "            \n",
    "        # For very small updates, use fewer steps to speed up processing\n",
    "        if len(new_data) == 1 and not (significant_change or volatility_increased):\n",
    "            vi_steps = int(vi_steps * 0.7)  # 30% fewer steps for minor updates\n",
    "            \n",
    "        # Fit the model with the determined number of steps\n",
    "        print(f\"Updating model with {len(self.observed_time_series)} points and {vi_steps} VI steps\")\n",
    "        self.posterior = self._fit_variational_inference(vi_steps)\n",
    "        \n",
    "        # Verify successful fit\n",
    "        if self.posterior is None:\n",
    "            print(\"Failed to fit variational inference during update\")\n",
    "            return False\n",
    "            \n",
    "        # Report success with detailed timing information\n",
    "        print(\n",
    "            f\"[{datetime.now().isoformat()}] Successfully updated model v{self.model_version} with {len(new_data)} new data points\")\n",
    "            \n",
    "        # Simulate an immediate forecast to update internal stat tracking\n",
    "        self.forecast()\n",
    "        \n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating model: {e}\\n{traceback.format_exc()}\")\n",
    "        return False\n",
    "\n",
    "def _fallback_forecast(self) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Create a fallback forecast when the primary model fails.\n",
    "    Uses simple statistical methods for basic prediction.\n",
    "\n",
    "    :return: Tuple of (mean prediction, lower bound, upper bound)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if self.observed_time_series is not None:\n",
    "            data = self.observed_time_series.numpy()\n",
    "            # Use exponential weighted mean with short span for faster response\n",
    "            df = pd.Series(data)\n",
    "            # More weight to recent prices\n",
    "            mean_val = extract_scalar_from_prediction(df.ewm(span=3).mean().iloc[-1])\n",
    "\n",
    "            # Calculate dynamic std based on recent volatility\n",
    "            if len(data) >= 10:\n",
    "                recent_std = extract_scalar_from_prediction(df.tail(10).std())\n",
    "                volatility_factor = recent_std / mean_val if mean_val != 0 else 0.005\n",
    "                std = mean_val * volatility_factor\n",
    "            else:\n",
    "                std = mean_val * 0.005\n",
    "\n",
    "            lower_val = mean_val - 1.96 * std\n",
    "            upper_val = mean_val + 1.96 * std\n",
    "\n",
    "            # Round values for consistency\n",
    "            mean_val = safe_round(mean_val, 2)\n",
    "            lower_val = safe_round(lower_val, 2)\n",
    "            upper_val = safe_round(upper_val, 2)\n",
    "\n",
    "            return mean_val, lower_val, upper_val\n",
    "\n",
    "        # Last resort - use a reasonable default value\n",
    "        recent_avg = 103000.0\n",
    "        return safe_round(recent_avg, 2), safe_round(recent_avg * 0.99, 2), safe_round(recent_avg * 1.01, 2)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in fallback forecast: {e}\")\n",
    "        # Absolute last resort\n",
    "        recent_avg = 103000.0\n",
    "        return safe_round(recent_avg, 2), safe_round(recent_avg * 0.99, 2), safe_round(recent_avg * 1.01, 2)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
